{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Container inventory legacy app This project illustrates the integration of a legacy application with MQ and Kafka. The application manages the current container inventory for the shipment company, introduced in the end to end solution here. The JEE application is a 3 tiers architecture, used to manage container inventory. As part of those containers are the Reefer ones. So the approach is to use the legacy application to keep manage the inventory but use MQ or Change Data Capture to propagate the inventory updates to the microservice world. Component views MQ integration The first way to propagate data change is to have the legacy application sharing data via MQ. This is common as MQ is used as messaging services since multiple years: On the left of the figure, the container management microservice (implemented in a separate repository ) supports the Reefer container management and events processing. In the current use case, this component is producing events to a containers topic defined in Kafka / Event Streams. One of those events will be container in maintenance and container out of maintenance. It is also interrested to receive new \"Reefer added\" to the legacy inventory so it can assign order to container. On the right side of the figure, the Inventory app is a JEE application managing the container inventory. We did it in Java because we can partially reuse another code from previous contribution. But the most important thing, it is considered as a legacy app, using MQ as communication layer. It could have been done in other language running on mainframe for example. This app is listening to MQ to get container maintenance messages, and publishes container added message to queues. The MQ source connector is a component that gets messages from the queue, filters messages only about relevant to Reefer container and maps the message as a containerAdded event. This code is based on Event Stream MQ connector. The MQ sink connector is a component to process container events from Kafka container topic, filter only the container on maitenance and off maintenance events and propagate them to MQ for down stream processing. Using Change Data Capture The second integration is via the deployment of a change data capture solution, like debezium The microservice on the left, is listening to events from kafka topics The legacy application runs as is, to persist data to a DB server like DB2. The transaction log is monitored by a CDC agent, running in cluster to push data as events to a kafka topic Pre-requisites We assume we have the following pre-installed software: Docker CLI and docker engine on your development machine For approach 1, MQ running on IBM Cloud, or on IBM Cloud Private, or when running locally using IBM docker image. For approach 2, Debezium in docker. Maven to compile the JEE application. The JEE app is packaged as war file, and deployed on a OpenLiberty server. It could also have been deployed to WebSphere Application Server. We just used a very simple JEE app. IBM Event Stream deployed on public or private cloud, or using Kafka docker images. Sub projects The jee-inventory folder includes the Java based project for managing the container inventory. We make it very simple using maven and deployable on WebSphere or on Liberty. The cdc folder includes the configuration of Debezium and approach 2. Build and run More readings Developing Java applications for MQ just got easier with Maven","title":"Introduction"},{"location":"#container-inventory-legacy-app","text":"This project illustrates the integration of a legacy application with MQ and Kafka. The application manages the current container inventory for the shipment company, introduced in the end to end solution here. The JEE application is a 3 tiers architecture, used to manage container inventory. As part of those containers are the Reefer ones. So the approach is to use the legacy application to keep manage the inventory but use MQ or Change Data Capture to propagate the inventory updates to the microservice world.","title":"Container inventory legacy app"},{"location":"#component-views","text":"","title":"Component views"},{"location":"#mq-integration","text":"The first way to propagate data change is to have the legacy application sharing data via MQ. This is common as MQ is used as messaging services since multiple years: On the left of the figure, the container management microservice (implemented in a separate repository ) supports the Reefer container management and events processing. In the current use case, this component is producing events to a containers topic defined in Kafka / Event Streams. One of those events will be container in maintenance and container out of maintenance. It is also interrested to receive new \"Reefer added\" to the legacy inventory so it can assign order to container. On the right side of the figure, the Inventory app is a JEE application managing the container inventory. We did it in Java because we can partially reuse another code from previous contribution. But the most important thing, it is considered as a legacy app, using MQ as communication layer. It could have been done in other language running on mainframe for example. This app is listening to MQ to get container maintenance messages, and publishes container added message to queues. The MQ source connector is a component that gets messages from the queue, filters messages only about relevant to Reefer container and maps the message as a containerAdded event. This code is based on Event Stream MQ connector. The MQ sink connector is a component to process container events from Kafka container topic, filter only the container on maitenance and off maintenance events and propagate them to MQ for down stream processing.","title":"MQ integration"},{"location":"#using-change-data-capture","text":"The second integration is via the deployment of a change data capture solution, like debezium The microservice on the left, is listening to events from kafka topics The legacy application runs as is, to persist data to a DB server like DB2. The transaction log is monitored by a CDC agent, running in cluster to push data as events to a kafka topic","title":"Using Change Data Capture"},{"location":"#pre-requisites","text":"We assume we have the following pre-installed software: Docker CLI and docker engine on your development machine For approach 1, MQ running on IBM Cloud, or on IBM Cloud Private, or when running locally using IBM docker image. For approach 2, Debezium in docker. Maven to compile the JEE application. The JEE app is packaged as war file, and deployed on a OpenLiberty server. It could also have been deployed to WebSphere Application Server. We just used a very simple JEE app. IBM Event Stream deployed on public or private cloud, or using Kafka docker images.","title":"Pre-requisites"},{"location":"#sub-projects","text":"The jee-inventory folder includes the Java based project for managing the container inventory. We make it very simple using maven and deployable on WebSphere or on Liberty. The cdc folder includes the configuration of Debezium and approach 2.","title":"Sub projects"},{"location":"#build-and-run","text":"","title":"Build and run"},{"location":"#more-readings","text":"Developing Java applications for MQ just got easier with Maven","title":"More readings"},{"location":"creatingApp/","text":"Creating app from command line The goal of this article is to go over a test driven and devops approach to develop this service. To create the microprofile app we used the ibmcloud CLI. (version 2.2.0 used). Requirements Maven Java 8: Any compliant JVM should work. Java 8 JDK from Oracle Java 8 JDK from IBM (AIX, Linux, z/OS, IBM i) , or Download a Liberty server package that contains the IBM JDK (Windows, Linux) Step 1: Create the foundation Login to IBM Cloud, and set the target to an existing organization and space ibmcloud login -a https://cloud.ibm.com -u <username> ibmcloud target -o cent@us.ibm.com -s Cognitive Create the foundation for an app ibmcloud dev create Then select the appropriate options. For our case there are: * Backend Service / Web App * Java - MicroProfile / Java EE * Java Microservice with Eclipse MicroProfile and Java EE * IBM DevOps, deploy to Kubernetes containers Step 2: Build and Run To build and run the application: 1. mvn install 1. mvn liberty:run-server To run the application in Docker use the Docker file called Dockerfile : If you do not want to install Maven locally you can use Dockerfile-tools to build a container with Maven installed. Endpoints The application exposes the following endpoints: * Health endpoint: <host>:<port>/<contextRoot>/health The context root is set in the src/main/webapp/WEB-INF/ibm-web-ext.xml file. The ports are set in the pom.xml file and exposed to the CLI in the cli-config.yml file.","title":"Create foundations"},{"location":"creatingApp/#creating-app-from-command-line","text":"The goal of this article is to go over a test driven and devops approach to develop this service. To create the microprofile app we used the ibmcloud CLI. (version 2.2.0 used).","title":"Creating app from command line"},{"location":"creatingApp/#requirements","text":"Maven Java 8: Any compliant JVM should work. Java 8 JDK from Oracle Java 8 JDK from IBM (AIX, Linux, z/OS, IBM i) , or Download a Liberty server package that contains the IBM JDK (Windows, Linux)","title":"Requirements"},{"location":"creatingApp/#step-1-create-the-foundation","text":"Login to IBM Cloud, and set the target to an existing organization and space ibmcloud login -a https://cloud.ibm.com -u <username> ibmcloud target -o cent@us.ibm.com -s Cognitive Create the foundation for an app ibmcloud dev create Then select the appropriate options. For our case there are: * Backend Service / Web App * Java - MicroProfile / Java EE * Java Microservice with Eclipse MicroProfile and Java EE * IBM DevOps, deploy to Kubernetes containers","title":"Step 1: Create the foundation"},{"location":"creatingApp/#step-2-build-and-run","text":"To build and run the application: 1. mvn install 1. mvn liberty:run-server To run the application in Docker use the Docker file called Dockerfile : If you do not want to install Maven locally you can use Dockerfile-tools to build a container with Maven installed.","title":"Step 2: Build and  Run"},{"location":"creatingApp/#endpoints","text":"The application exposes the following endpoints: * Health endpoint: <host>:<port>/<contextRoot>/health The context root is set in the src/main/webapp/WEB-INF/ibm-web-ext.xml file. The ports are set in the pom.xml file and exposed to the CLI in the cli-config.yml file.","title":"Endpoints"},{"location":"run-icp/","text":"Run on IBM Cloud Private","title":"Run on IBM Cloud private"},{"location":"run-icp/#run-on-ibm-cloud-private","text":"","title":"Run on IBM Cloud Private"},{"location":"run-local/","text":"Run Locally Running MQ with docker Prepare the MQ run time Attention : Queue manager and queue data are saved in the filesystem. To avoid losing the queue manager and queue data, we use docker volumes. Volumes are attached to containers when they are run and persist after the container is deleted. The following command creates a volume name qm1data $ docker volume create qm1data The remote MQ clients use a Channel to communicate with the MQ manager over the network. We need to create a virtual docker network to support this communication. The command below creates such network: $ docker network create mq-brown-network The scripts runMQlocal uses docker and the IBM MQ docker image to run MQ as a daemon.","title":"Run locally with Minikube"},{"location":"run-local/#run-locally","text":"","title":"Run Locally"},{"location":"run-local/#running-mq-with-docker","text":"","title":"Running MQ with docker"},{"location":"run-local/#prepare-the-mq-run-time","text":"Attention : Queue manager and queue data are saved in the filesystem. To avoid losing the queue manager and queue data, we use docker volumes. Volumes are attached to containers when they are run and persist after the container is deleted. The following command creates a volume name qm1data $ docker volume create qm1data The remote MQ clients use a Channel to communicate with the MQ manager over the network. We need to create a virtual docker network to support this communication. The command below creates such network: $ docker network create mq-brown-network The scripts runMQlocal uses docker and the IBM MQ docker image to run MQ as a daemon.","title":"Prepare the MQ run time"},{"location":"tdd/","text":"Applying Test Driven Development","title":"Apply TDD"},{"location":"tdd/#applying-test-driven-development","text":"","title":"Applying Test Driven Development"}]}