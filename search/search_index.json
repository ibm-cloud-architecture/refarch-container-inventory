{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Container inventory legacy app This project illustrates how to do data integration from a legacy application to microservice using MQ and Apache Kafka. The application manages the current container inventory for the Shipping company, introduced in the event driven architecture end to end solution implementation. The JEE application is a three tiers architecture, used to manage any type of shipping container (Dry, FlatRack, OpenTop, OpenSide, Tunnel, Tank, Thermal, Reefer) inventory. The approach is to use this \"legacy application\" to keep managing the traditional container inventory but use MQ to propagate the inventory updates to the microservice world. The implementationi is really to demonstrate MQ to Kafka connector, the legacy application could be anything that publishes to MQ, so a mainframe application will do the same. The second interest in this application is the code example of integrating with MQ using JMS APIs. Component views A commons approach for existing application to integate with modern microservice is to share data via MQ. The solution we want to implement looks like the following figure: On the left of the figure, the reefer container management microservice (implemented in a separate repository ) supports the Reefer container management and the events processing. In the current use case, this component is producing events to a containers topic defined in Kafka / Event Streams. One of those events will be container in maintenance and container out of maintenance. The reefer management microservice is interrested to receive new \"Reefer added to the legacy inventory\" events so it can assign order to those newly added reefers. It is not interested in any other types of shipping container. On the right side of the figure, the Inventory app is a JEE application managing the container inventory. We did it in Java because we can partially reuse another code from previous contribution. But the most important thing, is to consider it as a legacy app, using MQ as communication layer. It could have been done in other language running on mainframe for example. This app is listening to MQ to get reefer maintenance messages, and publishes container added message to queues. The MQ source connector is a component that gets messages from the queue, filters messages only relevant to Reefer container and maps the message as a containerAdded event. This code is based on Event Stream MQ connector running within Kafka Connect framework. The MQ sink connector is a component to process container events from Kafka container topic, filter only the container on maintenance and off maintenance events and propagate them to MQ for down stream processing by the legacy application (update the inventory). Using Change Data Capture The second possible choise to replicate data from Legacy to microservice is to use a change data capture solution, like debezium or IBM data replication . The following diagram illustrates such approach: The microservice on the left, is listening to events from kafka topics, and also produces event to kafka. It persists its own data like the reefers state and the reefer's telemetries. The legacy application runs as is, to persist data to a DB2 server. The transaction log is monitored by a CDC agent, running in VM, and responsible to push thedata as events to a kafka topic. One table to one topic. As reefer maintenance events have interest in the legacy app, a data transformation agent is added to the solution, to listen to container events and call a SOAP end point to update the state of the container within the legacy inventory. We are detailing this approach in a separate note . Pre-requisites We assume we have the following pre-installed software: Docker CLI and docker engine on your local development machine MQ running on IBM Cloud, or when running locally using IBM docker image: We are providing a dockerfile and mq configuration to help you on this. See 'Create the MQ image with the queue configuration' section below. Maven to compile the JEE application. The JEE app is packaged as war file, and deployed on an OpenLiberty server. It could also have been deployed to traditional WebSphere Application Server. We just used a very simple JEE app in this solution, as the focus is more on the integration. This application is using JMS to MQ to send and listen to messages. IBM Event Stream deployed on public Cloud (see this instruction for configuring the service) or on Openshift or using the Strimzi opertor. Sub projects The jee-inventory folder includes the Java based project for managing the container inventory. We make it very simple using maven and deployable on WebSphere or on Liberty. The db2 includes the scripts and SQL DDL to create the database for the inventory and a docker image for running DB2 locally. The cdc folder includes the configuration of Debezium and for the CDC approach. The mq-config includes the MQ configuration and docker file to run MQ locally. Build and run See this separate note. Integration tests The folder scripts includes a simple add container scripts to post a new container to the web application. To create a Reefer with id C200 use: . / addContainer . sh C200 To validate the record is added to the database curl http : // localhost : 9080 / containers / C200 We should have enought to test CDC with Debezium. More readings Developing Java applications for MQ just got easier with Maven IBM InfoSphere Data Replication Product Tour Integrating IBM CDC Replication Engine with kafka","title":"Introduction"},{"location":"#container-inventory-legacy-app","text":"This project illustrates how to do data integration from a legacy application to microservice using MQ and Apache Kafka. The application manages the current container inventory for the Shipping company, introduced in the event driven architecture end to end solution implementation. The JEE application is a three tiers architecture, used to manage any type of shipping container (Dry, FlatRack, OpenTop, OpenSide, Tunnel, Tank, Thermal, Reefer) inventory. The approach is to use this \"legacy application\" to keep managing the traditional container inventory but use MQ to propagate the inventory updates to the microservice world. The implementationi is really to demonstrate MQ to Kafka connector, the legacy application could be anything that publishes to MQ, so a mainframe application will do the same. The second interest in this application is the code example of integrating with MQ using JMS APIs.","title":"Container inventory legacy app"},{"location":"#component-views","text":"A commons approach for existing application to integate with modern microservice is to share data via MQ. The solution we want to implement looks like the following figure: On the left of the figure, the reefer container management microservice (implemented in a separate repository ) supports the Reefer container management and the events processing. In the current use case, this component is producing events to a containers topic defined in Kafka / Event Streams. One of those events will be container in maintenance and container out of maintenance. The reefer management microservice is interrested to receive new \"Reefer added to the legacy inventory\" events so it can assign order to those newly added reefers. It is not interested in any other types of shipping container. On the right side of the figure, the Inventory app is a JEE application managing the container inventory. We did it in Java because we can partially reuse another code from previous contribution. But the most important thing, is to consider it as a legacy app, using MQ as communication layer. It could have been done in other language running on mainframe for example. This app is listening to MQ to get reefer maintenance messages, and publishes container added message to queues. The MQ source connector is a component that gets messages from the queue, filters messages only relevant to Reefer container and maps the message as a containerAdded event. This code is based on Event Stream MQ connector running within Kafka Connect framework. The MQ sink connector is a component to process container events from Kafka container topic, filter only the container on maintenance and off maintenance events and propagate them to MQ for down stream processing by the legacy application (update the inventory).","title":"Component views"},{"location":"#using-change-data-capture","text":"The second possible choise to replicate data from Legacy to microservice is to use a change data capture solution, like debezium or IBM data replication . The following diagram illustrates such approach: The microservice on the left, is listening to events from kafka topics, and also produces event to kafka. It persists its own data like the reefers state and the reefer's telemetries. The legacy application runs as is, to persist data to a DB2 server. The transaction log is monitored by a CDC agent, running in VM, and responsible to push thedata as events to a kafka topic. One table to one topic. As reefer maintenance events have interest in the legacy app, a data transformation agent is added to the solution, to listen to container events and call a SOAP end point to update the state of the container within the legacy inventory. We are detailing this approach in a separate note .","title":"Using Change Data Capture"},{"location":"#pre-requisites","text":"We assume we have the following pre-installed software: Docker CLI and docker engine on your local development machine MQ running on IBM Cloud, or when running locally using IBM docker image: We are providing a dockerfile and mq configuration to help you on this. See 'Create the MQ image with the queue configuration' section below. Maven to compile the JEE application. The JEE app is packaged as war file, and deployed on an OpenLiberty server. It could also have been deployed to traditional WebSphere Application Server. We just used a very simple JEE app in this solution, as the focus is more on the integration. This application is using JMS to MQ to send and listen to messages. IBM Event Stream deployed on public Cloud (see this instruction for configuring the service) or on Openshift or using the Strimzi opertor.","title":"Pre-requisites"},{"location":"#sub-projects","text":"The jee-inventory folder includes the Java based project for managing the container inventory. We make it very simple using maven and deployable on WebSphere or on Liberty. The db2 includes the scripts and SQL DDL to create the database for the inventory and a docker image for running DB2 locally. The cdc folder includes the configuration of Debezium and for the CDC approach. The mq-config includes the MQ configuration and docker file to run MQ locally.","title":"Sub projects"},{"location":"#build-and-run","text":"See this separate note.","title":"Build and run"},{"location":"#integration-tests","text":"The folder scripts includes a simple add container scripts to post a new container to the web application. To create a Reefer with id C200 use: . / addContainer . sh C200 To validate the record is added to the database curl http : // localhost : 9080 / containers / C200 We should have enought to test CDC with Debezium.","title":"Integration tests"},{"location":"#more-readings","text":"Developing Java applications for MQ just got easier with Maven IBM InfoSphere Data Replication Product Tour Integrating IBM CDC Replication Engine with kafka","title":"More readings"},{"location":"cdc/","text":"Change Data Capture","title":"CDC - Kafka"},{"location":"cdc/#change-data-capture","text":"","title":"Change Data Capture"},{"location":"creatingApp/","text":"Creating app from command line The goal of this article is to go over a test driven and devops approach to develop this service. To create the microprofile app we used the ibmcloud CLI. (version 2.2.0 used). Requirements Maven Java 8: Any compliant JVM should work. Java 8 JDK from Oracle Java 8 JDK from IBM (AIX, Linux, z/OS, IBM i) , or Download a Liberty server package that contains the IBM JDK (Windows, Linux) Step 1: Create the foundation Login to IBM Cloud, and set the target to an existing organization and space ibmcloud login - a https : // cloud . ibm . com - u < username > ibmcloud target - o cent @us . ibm . com - s Cognitive Create the foundation for an app ibmcloud dev create Then select the appropriate options. For our case there are: * Backend Service / Web App * Java - MicroProfile / Java EE * Java Microservice with Eclipse MicroProfile and Java EE * IBM DevOps , deploy to Kubernetes containers Step 2: Build and Run To build and run the application: mvn install mvn liberty:run-server To run the application in Docker use the Docker file called Dockerfile : If you do not want to install Maven locally you can use Dockerfile-tools to build a container with Maven installed. Endpoints The application exposes the following endpoints: * Health endpoint: <host>:<port>/<contextRoot>/health The context root is set in the src/main/webapp/WEB-INF/ibm-web-ext.xml file. The ports are set in the pom.xml file and exposed to the CLI in the cli-config.yml file.","title":"Creating app from command line"},{"location":"creatingApp/#creating-app-from-command-line","text":"The goal of this article is to go over a test driven and devops approach to develop this service. To create the microprofile app we used the ibmcloud CLI. (version 2.2.0 used).","title":"Creating app from command line"},{"location":"creatingApp/#requirements","text":"Maven Java 8: Any compliant JVM should work. Java 8 JDK from Oracle Java 8 JDK from IBM (AIX, Linux, z/OS, IBM i) , or Download a Liberty server package that contains the IBM JDK (Windows, Linux)","title":"Requirements"},{"location":"creatingApp/#step-1-create-the-foundation","text":"Login to IBM Cloud, and set the target to an existing organization and space ibmcloud login - a https : // cloud . ibm . com - u < username > ibmcloud target - o cent @us . ibm . com - s Cognitive Create the foundation for an app ibmcloud dev create Then select the appropriate options. For our case there are: * Backend Service / Web App * Java - MicroProfile / Java EE * Java Microservice with Eclipse MicroProfile and Java EE * IBM DevOps , deploy to Kubernetes containers","title":"Step 1: Create the foundation"},{"location":"creatingApp/#step-2-build-and-run","text":"To build and run the application: mvn install mvn liberty:run-server To run the application in Docker use the Docker file called Dockerfile : If you do not want to install Maven locally you can use Dockerfile-tools to build a container with Maven installed.","title":"Step 2: Build and  Run"},{"location":"creatingApp/#endpoints","text":"The application exposes the following endpoints: * Health endpoint: <host>:<port>/<contextRoot>/health The context root is set in the src/main/webapp/WEB-INF/ibm-web-ext.xml file. The ports are set in the pom.xml file and exposed to the CLI in the cli-config.yml file.","title":"Endpoints"},{"location":"debezium-postgresql/","text":"Streaming PostgreSQL Updates to Kafka with Debezium This guide will help you get up and running with Kafka Connect to stream PostgreSQL database changes to a Kafka topic. It will guide you through the installation and configuration of Kafka, Kafka Connect, Debezium & PostgreSQL. Note: This guide has only been tested using Docker Desktop for Mac. Results may vary using other Kubernetes cluster types. This guide assumes that you have the following: Access to a Kubernetes Cluster Helm (w/ Tiller) installed on the Kubernetes Cluster Helm Incubator repository enabled cURL, Postman or another HTTP client If you do not meet the prerequisites, please see the following links: Getting Started with Kubernetes with Docker on Mac Install Helm Enable Helm Incubator repository Install Postman Initial Setup To keep this tutorial isolated from other application running in your Kubernetes cluster and to cleanup easier, we will create a separate namespace for the new resources. $ kubectl create namespace kafka-connect-tutorial (Optional) You may configure your Kubernetes context's default namespace to kafka-connect-tutorial using the following command: $ kubectl config set-context --current --namespace kafka-connect-tutorial Install Kafka & Zookeeper This section will guide you through the installation of Kafka & Zookeeper. You will also deploy a Kafka client pod to interact with the Kafka Cluster, as well as configure 3 Kafka Topics that will be used by Kafka Connect. Install Kafka & Zookeeper to your namespace using the Incubator Helm Chart. $ helm install --name kafka --namespace kafka-connect-tutorial incubator/kafka --set external.enabled = true Deploy a Kafka Connect client container to your cluster by creating a file in your workspace named kafka-client-deploy.yaml with the following contents: # kafka-client-deploy.yaml apiVersion : v1 kind : Pod metadata : name : kafka-client spec : containers : - name : kafka-client image : confluentinc/cp-kafka:5.0.1 command : - sh - -c - \"exec tail -f /dev/null\" Execute the following command to deploy the Kafka Client Pod: $ kubectl create -f kafka-client-deploy.yaml -n kafka-connect-tutorial Create the Kafka Connect Topics using the following commands: $ kubectl -n kafka-connect-tutorial exec kafka-client -- kafka-topics --zookeeper kafka-zookeeper:2181 --topic connect-offsets --create --partitions 1 --replication-factor 1 $ kubectl -n kafka-connect-tutorial exec kafka-client -- kafka-topics --zookeeper kafka-zookeeper:2181 --topic connect-configs --create --partitions 1 --replication-factor 1 $ kubectl -n kafka-connect-tutorial exec kafka-client -- kafka-topics --zookeeper kafka-zookeeper:2181 --topic connect-status --create --partitions 1 --replication-factor 1 Install Kafka Connect This section will guide you through the installation of Kafka Connect using the Debezium Kafka Connect Docker Image. As part of this installation, you will create a NodePort service to expose the Kafka Connect API. This service will be available on port 30500 of your cluster nodes. If you are using Docker Desktop, this will be http://localhost:30500. Create a file named kafka-connect-deploy.yaml in your workspace and add the following contents: # kafka-connect-deploy.yaml apiVersion : apps/v1 kind : Deployment metadata : name : kafkaconnect-deploy labels : app : kafkaconnect spec : replicas : 1 selector : matchLabels : app : kafkaconnect template : metadata : labels : app : kafkaconnect spec : containers : - name : kafkaconnect-container image : debezium/connect:0.10.0.CR1 readinessProbe : httpGet : path : / port : 8083 livenessProbe : httpGet : path : / port : 8083 env : - name : BOOTSTRAP_SERVERS value : kafka:9092 - name : GROUP_ID value : \"1\" - name : OFFSET_STORAGE_TOPIC value : connect-offsets - name : CONFIG_STORAGE_TOPIC value : connect-configs - name : STATUS_STORAGE_TOPIC value : connect-status ports : - containerPort : 8083 --- apiVersion : v1 kind : Service metadata : name : kafkaconnect-service labels : app : kafkaconnect-service spec : type : NodePort ports : - name : kafkaconnect protocol : TCP port : 8083 nodePort : 30500 selector : app : kafkaconnect Deploy Kafka Connect with the following command: $ kubectl create -f kafka-connect-deploy.yaml --namespace kafka-connect-tutorial Install PostgreSQL This section will guide you through the installation of PostgreSQL using the Stable Helm Chart. You will also add some additional configuration for PostgreSQL necessary for Debezium to read the PostgreSQL transaction log. PostgreSQL will be available on port 30600 of your cluster nodes. If you are using Docker Desktop, this will be http://localhost:30600. Create a PostgreSQL configuration necessary for Debezium. Create a file in your workspace named extended.conf with the following contents: # extended . conf wal_level = logical max_wal_senders = 1 max_replication_slots = 1 Create a ConfigMap from the extended.conf file with the following command: $ kubectl create configmap --namespace kafka-connect-tutorial --from-file = extended.conf postgresql-config Install PostgreSQL using the Stable Helm Chart with the following command: $ helm install --name postgres --namespace kafka-connect-tutorial --set extendedConfConfigMap = postgresql-config --set service.type = NodePort --set service.nodePort = 30600 --set postgresqlPassword = passw0rd stable/postgresql Add Sample Data to PostgreSQL Open a shell in the Postgres container. $ kubectl exec --namespace kafka-connect-tutorial -it postgres-postgresql-0 -- /bin/sh Login to Postgres with the following command, entering the password passw0rd when prompted. $ psql --user postgres Create a table named containers . CREATE TABLE containers ( containerid VARCHAR ( 30 ) NOT NULL , type VARCHAR ( 20 ), status VARCHAR ( 20 ), brand VARCHAR ( 50 ), capacity DECIMAL , CREATIONDATE TIMESTAMP DEFAULT CURRENT_TIMESTAMP , UPDATEDATE TIMESTAMP DEFAULT CURRENT_TIMESTAMP , PRIMARY KEY ( containerid )); Insert data into the table. INSERT INTO containers ( containerid , type , status , brand , capacity ) VALUES ( 'C01' , 'Reefer' , 'Operational' , 'containerbrand' , 20 ), ( 'C02' , 'Dry' , 'Operational' , 'containerbrand' , 20 ), ( 'C03' , 'Dry' , 'Operational' , 'containerbrand' , 40 ), ( 'C04' , 'FlatRack' , 'Operational' , 'containerbrand' , 40 ), ( 'C05' , 'OpenTop' , 'Operational' , 'containerbrand' , 40 ), ( 'C06' , 'OpenSide' , 'Operational' , 'containerbrand' , 40 ), ( 'C07' , 'Tunnel' , 'Operational' , 'containerbrand' , 40 ), ( 'C08' , 'Tank' , 'Operational' , 'containerbrand' , 40 ), ( 'C09' , 'Thermal' , 'Operational' , 'containerbrand' , 20 ); Configure the Debezium PostgreSQL connector This section will show you how to configure the Debezium PostgreSQL connector. Using your HTTP client (cURL shown), make the following request to the Kafka Connect API. This will configure a new Debezium PostgreSQL connector. This connector monitors the pgoutput stream for operations on the containers table. Note: If you are not using Docker Desktop, please set localhost to the hostname/IP of one of your cluster nodes. Note: If you did not follow the Add Sample Data to PostgreSQL section, replace \"public.containers\" with the name of your table. curl -X POST \\ http://localhost:30500/connectors \\ -H 'Content-Type: application/json' \\ -d '{ \"name\": \"containers-connector\", \"config\": { \"connector.class\": \"io.debezium.connector.postgresql.PostgresConnector\", \"plugin.name\": \"pgoutput\", \"database.hostname\": \"postgres-postgresql\", \"database.port\": \"5432\", \"database.user\": \"postgres\", \"database.password\": \"passw0rd\", \"database.dbname\": \"postgres\", \"database.server.name\": \"postgres\", \"table.whitelist\": \"public.containers\" } }' List the Kafka Topics, showing your newly created topic $ kubectl -n kafka-connect-tutorial exec kafka-client -- kafka-topics --zookeeper kafka-zookeeper:2181 --list Tail the Kafka postgres.public.containers topic to show database transactions being written to the topic from Kafka Connect. Note: Change postgres.public.containers if you are not using the sample database data $ kubectl -n kafka-connect-tutorial exec kafka-client -- kafka-console-consumer --topic postgres.public.containers --from-beginning --bootstrap-server kafka:9092 You may continue to make Create, Update and Delete transactions to the containers table, these changes will appear as messages in the Kafka topic. Cleanup This section will help you remove all of the resources created during this tutorial. Delete the Kafka Helm Release $ helm delete kafka --purge Delete the PostgreSQL Helm Release $ helm delete postgres --purge Delete the kafka-connect-tutorial namespace $ kubectl delete namespace kafka-connect-tutorial References Debezium Tutorial PostgreSQL plugins/configuration required for Debezium","title":"Postgres - Debezium to kafka"},{"location":"debezium-postgresql/#streaming-postgresql-updates-to-kafka-with-debezium","text":"This guide will help you get up and running with Kafka Connect to stream PostgreSQL database changes to a Kafka topic. It will guide you through the installation and configuration of Kafka, Kafka Connect, Debezium & PostgreSQL. Note: This guide has only been tested using Docker Desktop for Mac. Results may vary using other Kubernetes cluster types. This guide assumes that you have the following: Access to a Kubernetes Cluster Helm (w/ Tiller) installed on the Kubernetes Cluster Helm Incubator repository enabled cURL, Postman or another HTTP client If you do not meet the prerequisites, please see the following links: Getting Started with Kubernetes with Docker on Mac Install Helm Enable Helm Incubator repository Install Postman","title":"Streaming PostgreSQL Updates to Kafka with Debezium"},{"location":"debezium-postgresql/#initial-setup","text":"To keep this tutorial isolated from other application running in your Kubernetes cluster and to cleanup easier, we will create a separate namespace for the new resources. $ kubectl create namespace kafka-connect-tutorial (Optional) You may configure your Kubernetes context's default namespace to kafka-connect-tutorial using the following command: $ kubectl config set-context --current --namespace kafka-connect-tutorial","title":"Initial Setup"},{"location":"debezium-postgresql/#install-kafka-zookeeper","text":"This section will guide you through the installation of Kafka & Zookeeper. You will also deploy a Kafka client pod to interact with the Kafka Cluster, as well as configure 3 Kafka Topics that will be used by Kafka Connect. Install Kafka & Zookeeper to your namespace using the Incubator Helm Chart. $ helm install --name kafka --namespace kafka-connect-tutorial incubator/kafka --set external.enabled = true Deploy a Kafka Connect client container to your cluster by creating a file in your workspace named kafka-client-deploy.yaml with the following contents: # kafka-client-deploy.yaml apiVersion : v1 kind : Pod metadata : name : kafka-client spec : containers : - name : kafka-client image : confluentinc/cp-kafka:5.0.1 command : - sh - -c - \"exec tail -f /dev/null\" Execute the following command to deploy the Kafka Client Pod: $ kubectl create -f kafka-client-deploy.yaml -n kafka-connect-tutorial Create the Kafka Connect Topics using the following commands: $ kubectl -n kafka-connect-tutorial exec kafka-client -- kafka-topics --zookeeper kafka-zookeeper:2181 --topic connect-offsets --create --partitions 1 --replication-factor 1 $ kubectl -n kafka-connect-tutorial exec kafka-client -- kafka-topics --zookeeper kafka-zookeeper:2181 --topic connect-configs --create --partitions 1 --replication-factor 1 $ kubectl -n kafka-connect-tutorial exec kafka-client -- kafka-topics --zookeeper kafka-zookeeper:2181 --topic connect-status --create --partitions 1 --replication-factor 1","title":"Install Kafka &amp; Zookeeper"},{"location":"debezium-postgresql/#install-kafka-connect","text":"This section will guide you through the installation of Kafka Connect using the Debezium Kafka Connect Docker Image. As part of this installation, you will create a NodePort service to expose the Kafka Connect API. This service will be available on port 30500 of your cluster nodes. If you are using Docker Desktop, this will be http://localhost:30500. Create a file named kafka-connect-deploy.yaml in your workspace and add the following contents: # kafka-connect-deploy.yaml apiVersion : apps/v1 kind : Deployment metadata : name : kafkaconnect-deploy labels : app : kafkaconnect spec : replicas : 1 selector : matchLabels : app : kafkaconnect template : metadata : labels : app : kafkaconnect spec : containers : - name : kafkaconnect-container image : debezium/connect:0.10.0.CR1 readinessProbe : httpGet : path : / port : 8083 livenessProbe : httpGet : path : / port : 8083 env : - name : BOOTSTRAP_SERVERS value : kafka:9092 - name : GROUP_ID value : \"1\" - name : OFFSET_STORAGE_TOPIC value : connect-offsets - name : CONFIG_STORAGE_TOPIC value : connect-configs - name : STATUS_STORAGE_TOPIC value : connect-status ports : - containerPort : 8083 --- apiVersion : v1 kind : Service metadata : name : kafkaconnect-service labels : app : kafkaconnect-service spec : type : NodePort ports : - name : kafkaconnect protocol : TCP port : 8083 nodePort : 30500 selector : app : kafkaconnect Deploy Kafka Connect with the following command: $ kubectl create -f kafka-connect-deploy.yaml --namespace kafka-connect-tutorial","title":"Install Kafka Connect"},{"location":"debezium-postgresql/#install-postgresql","text":"This section will guide you through the installation of PostgreSQL using the Stable Helm Chart. You will also add some additional configuration for PostgreSQL necessary for Debezium to read the PostgreSQL transaction log. PostgreSQL will be available on port 30600 of your cluster nodes. If you are using Docker Desktop, this will be http://localhost:30600. Create a PostgreSQL configuration necessary for Debezium. Create a file in your workspace named extended.conf with the following contents: # extended . conf wal_level = logical max_wal_senders = 1 max_replication_slots = 1 Create a ConfigMap from the extended.conf file with the following command: $ kubectl create configmap --namespace kafka-connect-tutorial --from-file = extended.conf postgresql-config Install PostgreSQL using the Stable Helm Chart with the following command: $ helm install --name postgres --namespace kafka-connect-tutorial --set extendedConfConfigMap = postgresql-config --set service.type = NodePort --set service.nodePort = 30600 --set postgresqlPassword = passw0rd stable/postgresql","title":"Install PostgreSQL"},{"location":"debezium-postgresql/#add-sample-data-to-postgresql","text":"Open a shell in the Postgres container. $ kubectl exec --namespace kafka-connect-tutorial -it postgres-postgresql-0 -- /bin/sh Login to Postgres with the following command, entering the password passw0rd when prompted. $ psql --user postgres Create a table named containers . CREATE TABLE containers ( containerid VARCHAR ( 30 ) NOT NULL , type VARCHAR ( 20 ), status VARCHAR ( 20 ), brand VARCHAR ( 50 ), capacity DECIMAL , CREATIONDATE TIMESTAMP DEFAULT CURRENT_TIMESTAMP , UPDATEDATE TIMESTAMP DEFAULT CURRENT_TIMESTAMP , PRIMARY KEY ( containerid )); Insert data into the table. INSERT INTO containers ( containerid , type , status , brand , capacity ) VALUES ( 'C01' , 'Reefer' , 'Operational' , 'containerbrand' , 20 ), ( 'C02' , 'Dry' , 'Operational' , 'containerbrand' , 20 ), ( 'C03' , 'Dry' , 'Operational' , 'containerbrand' , 40 ), ( 'C04' , 'FlatRack' , 'Operational' , 'containerbrand' , 40 ), ( 'C05' , 'OpenTop' , 'Operational' , 'containerbrand' , 40 ), ( 'C06' , 'OpenSide' , 'Operational' , 'containerbrand' , 40 ), ( 'C07' , 'Tunnel' , 'Operational' , 'containerbrand' , 40 ), ( 'C08' , 'Tank' , 'Operational' , 'containerbrand' , 40 ), ( 'C09' , 'Thermal' , 'Operational' , 'containerbrand' , 20 );","title":"Add Sample Data to PostgreSQL"},{"location":"debezium-postgresql/#configure-the-debezium-postgresql-connector","text":"This section will show you how to configure the Debezium PostgreSQL connector. Using your HTTP client (cURL shown), make the following request to the Kafka Connect API. This will configure a new Debezium PostgreSQL connector. This connector monitors the pgoutput stream for operations on the containers table. Note: If you are not using Docker Desktop, please set localhost to the hostname/IP of one of your cluster nodes. Note: If you did not follow the Add Sample Data to PostgreSQL section, replace \"public.containers\" with the name of your table. curl -X POST \\ http://localhost:30500/connectors \\ -H 'Content-Type: application/json' \\ -d '{ \"name\": \"containers-connector\", \"config\": { \"connector.class\": \"io.debezium.connector.postgresql.PostgresConnector\", \"plugin.name\": \"pgoutput\", \"database.hostname\": \"postgres-postgresql\", \"database.port\": \"5432\", \"database.user\": \"postgres\", \"database.password\": \"passw0rd\", \"database.dbname\": \"postgres\", \"database.server.name\": \"postgres\", \"table.whitelist\": \"public.containers\" } }' List the Kafka Topics, showing your newly created topic $ kubectl -n kafka-connect-tutorial exec kafka-client -- kafka-topics --zookeeper kafka-zookeeper:2181 --list Tail the Kafka postgres.public.containers topic to show database transactions being written to the topic from Kafka Connect. Note: Change postgres.public.containers if you are not using the sample database data $ kubectl -n kafka-connect-tutorial exec kafka-client -- kafka-console-consumer --topic postgres.public.containers --from-beginning --bootstrap-server kafka:9092 You may continue to make Create, Update and Delete transactions to the containers table, these changes will appear as messages in the Kafka topic.","title":"Configure the Debezium PostgreSQL connector"},{"location":"debezium-postgresql/#cleanup","text":"This section will help you remove all of the resources created during this tutorial. Delete the Kafka Helm Release $ helm delete kafka --purge Delete the PostgreSQL Helm Release $ helm delete postgres --purge Delete the kafka-connect-tutorial namespace $ kubectl delete namespace kafka-connect-tutorial","title":"Cleanup"},{"location":"debezium-postgresql/#references","text":"Debezium Tutorial PostgreSQL plugins/configuration required for Debezium","title":"References"},{"location":"eventstreams-debezium/","text":"Using Debezium with Event Streams on OpenShift This guide will show you how to use Debezium with Event Streams. It will do this by demonstrating streaming PostgreSQL changes to an Event Streams topic. This guide assumes you have the following: Installation of Event Streams on OpenShift OpenShift CLI & Helm CLI on your client machine Docker installed on your client machine Download the Kafka Connect Configuration from Event Streams & Configure Event Streams This section will guide you through downloading the Kafka Connect configuration generated by Event Streams. Log in to the IBM Cloud Private Web Console & navigate to your Event Streams Helm Release from Workloads -> Helm Releases. Launch the Event Streams UI - Inside of the Event Streams Helm Release view, click the Launch button and select admin-ui-https . From the Event Streams UI, Navigate to Toolbox -> Set up a Kafka Connect environment and follow the instructions on screen. These instructions should guide you through creating topics for Kafka Connect, generating an API key and provide you with your Kafka Connect configuration. Build the Kafka Connect Image This section will guide you through building a Kafka Connect Image that includes the Debezium PostgreSQL connector. This section will use the package downloaded in the previous section. Unzip the kafkaconnect.zip file that was downloaded $ unzip kafkaconnect.zip -d kafkaconnect Delete the zipped kafkaconnect package $ rm kafkaconnect.zip Download the Debezium PostgreSQL Connector (v0.10.0.CR1) package from Maven Central $ wget https://repo1.maven.org/maven2/io/debezium/debezium-connector-postgres/0.10.0.CR1/debezium-connector-postgres-0.10.0.CR1-plugin.tar.gz Untar the Debezium PostgreSQL Connector into the kafkaconnect/connectors directory $ tar -xvzf debezium-connector-postgres-0.10.0.CR1-plugin.tar.gz -C kafkaconnect/connectors Delete the Debezium PostgreSQL Connector tarball $ rm debezium-connector-postgres-0.10.0.CR1-plugin.tar.gz Build the Kafka Connect image with Docker $ docker build -t kafka-connect-debezium-postgres:0.0.1 kafkaconnect Upload the Kafka Connect Image to OpenShift This section will guide you through uploading the Kafka Connect image to OpenShift. This will require SSH access to a cluster node. For alternative methods, please refer to the OpenShift documentation . Package the Docker image built in the last section as a tarball $ docker save kafka-connect-debezium-postgres:0.0.1 --output kafka-connect-debezium-postgres-0-0-1.tar Upload the image tarball to one of the cluster nodes using SCP. For example a cluster node at 192.168.10.2 using user root $ scp kafka-connect-debezium-postgres-0-0-1.tar root@192.168.10.2:/root/ SSH into the cluster node $ ssh root@192.168.10.2 Load the uploaded image tarball into the local Docker repository $ docker load --input kafka-connect-debezium-postgres-0-0-1.tar Login to your OpenShift cluster with the oc command $ oc login Login to the OpenShift Docker registry, this assumes that the registry is installed at the default location docker-registry.default.svc.cluster.local:5000 $ docker login -u any_value -p $( oc whoami -t ) docker-registry.default.svc.cluster.local:5000 Tag the Kafka Connect image with an appropriate registry location. Replace your-namespace-here with the namespace where you will deploy Kafka Connect, if deploying to another namespace $ docker tag kafka-connect-debezium-postgres:0.0.1 docker-registry.default.svc.cluster.local:5000/your-namespace-here/kafka-connect-debezium-postgres:0.0.1 Push the image up to the OpenShift Docker registry, replacing your-namespace-here with the namespace you specified in the last step $ docker push docker-registry.default.svc.cluster.local:5000/your-namespace-here/kafka-connect-debezium-postgres:0.0.1 Deploy Kafka Connect on OpenShift This section will guide you through deploying Kafka Connect on OpenShift using the image that you uploaded in the last section. Kafka Connect will be exposed as a NodePort service on port 30500 of your cluster nodes. Return to the workspace that contains the kafkaconnect directory that was downloaded in the first section, and login to your OpenShift cluster $ oc login Upload connect-distributed.properties as a Secret in the OpenShift namespace where you will deploy Kafka Connect (replacing your-namespace-here ) $ oc create secret generic --namespace your-namespace-here --from-file kafkaconnect/config/connect-distributed.properties connect-distributed-config Upload connect-log4j.properties as a ConfigMap in the OpenShift namespace where you will deploy Kafka Connect (replacing your-namespace-here ) $ oc create configmap --namespace your-namespace-here --from-file kafkaconnect/config/connect-log4j.properties connect-log4j-config Create a file in your workspace named kafka-connect-deploy.yaml with the following contents. Replace the value of spec.template.spec.containers.0.image with the image name that you uploaded in the last section # Deployment apiVersion : apps/v1 kind : Deployment metadata : name : kafkaconnect-deploy labels : app : kafkaconnect spec : replicas : 1 selector : matchLabels : app : kafkaconnect template : metadata : labels : app : kafkaconnect spec : securityContext : runAsNonRoot : true runAsUser : 5000 containers : - name : kafkaconnect-container image : docker-registry.default.svc.cluster.local:5000/your-namespace-here/kafka-connect-debezium-postgres:0.0.1 readinessProbe : httpGet : path : / port : 8083 livenessProbe : httpGet : path : / port : 8083 ports : - containerPort : 8083 volumeMounts : - name : connect-config mountPath : /opt/kafka/config/connect-distributed.properties subPath : connect-distributed.properties - name : connect-log4j mountPath : /opt/kafka/config/connect-log4j.properties subPath : connect-log4j.properties volumes : - name : connect-config secret : secretName : connect-distributed-config - name : connect-log4j configMap : name : connect-log4j-config --- # Service apiVersion : v1 kind : Service metadata : name : kafkaconnect-service labels : app : kafkaconnect-service spec : type : NodePort ports : - name : kafkaconnect protocol : TCP port : 8083 nodePort : 30500 selector : app : kafkaconnect Deploy Kafka Connect (replacing your-namespace-here ) $ oc create --namespace your-namespace-here -f kafka-connect-deploy.yaml Configure or Install PostgreSQL This section will give you the minimum required configuration for PostgreSQL by Debezium. If you do not have PostgreSQL, you can proceed to the Installation section. Configure PostgreSQL If you already have PostgreSQL installed, you must enable the logical decoding feature of PostgreSQL (v. >=9.4) and enable a logical decoding output plugin. PostgreSQL v.10+ includes the pgoutput logical decoding output plugin by default. If you are on a lower version, you must first install either decoderbufs or wal2json . More information can be found on the Debezium website here After you have an acceptable local decoding output plugin installed, refer to the Debezium documentation to complete the configuration of your PostgreSQL server https://debezium.io/documentation/reference/0.10/connectors/postgresql.html#server-configuration Install PostgreSQL If you do not have PostgreSQL installed, you may use a Helm Chart to get a working PostgreSQL instance on your cluster. This section will guide you through the installation of PostgreSQL using the Stable Helm Chart. You will also add some additional configuration for PostgreSQL necessary for Debezium to read the PostgreSQL transaction log. PostgreSQL will be available on port 30600 of your cluster nodes. The superuser will be postgres , password is passw0rd and an initial database name of postgres . Create a PostgreSQL configuration necessary for Debezium. Create a file in your workspace named extended.conf with the following contents: # extended . conf wal_level = logical max_wal_senders = 1 max_replication_slots = 1 Create a ConfigMap from the extended.conf file with the following command (replacing your-namespace-here ) $ oc create configmap --namespace your-namepace-here --from-file = extended.conf postgresql-config Install PostgreSQL using the Stable Helm Chart with the following command (replacing your-namespace-here ). For more configuration options, refer to the PostgreSQL Helm Chart $ helm install --name postgres --namespace your-namespace-here --set extendedConfConfigMap = postgresql-config --set service.type = NodePort --set service.nodePort = 30600 --set postgresqlPassword = passw0rd --set securityContext.runAsUser = 5000 --set volumePermissions.securityContext.runAsUser = 5000 stable/postgresql --tls Finally, connect to your PostgreSQL instance with your favorite PostgreSQL client and create a table with data. Note that the psql client is installed on the postgres container. You may exec into the postgres container and use psql from there. $ oc exec --namespace your-namespace-here -it postgres-postgresql-0 -- /bin/sh $ psql login --user postgres Add Sample Data to PostgreSQL Open a shell in the Postgres container. $ kubectl exec --namespace your-namespace-here -it postgres-postgresql-0 -- /bin/sh Login to Postgres with the following command, entering the password passw0rd when prompted (this is the password set from the helm install command in the previous section). $ psql --user postgres Create a table named containers . CREATE TABLE containers ( containerid VARCHAR ( 30 ) NOT NULL , type VARCHAR ( 20 ), status VARCHAR ( 20 ), brand VARCHAR ( 50 ), capacity DECIMAL , CREATIONDATE TIMESTAMP DEFAULT CURRENT_TIMESTAMP , UPDATEDATE TIMESTAMP DEFAULT CURRENT_TIMESTAMP , PRIMARY KEY ( containerid )); Insert data into the table. INSERT INTO containers ( containerid , type , status , brand , capacity ) VALUES ( 'C01' , 'Reefer' , 'Operational' , 'containerbrand' , 20 ), ( 'C02' , 'Dry' , 'Operational' , 'containerbrand' , 20 ), ( 'C03' , 'Dry' , 'Operational' , 'containerbrand' , 40 ), ( 'C04' , 'FlatRack' , 'Operational' , 'containerbrand' , 40 ), ( 'C05' , 'OpenTop' , 'Operational' , 'containerbrand' , 40 ), ( 'C06' , 'OpenSide' , 'Operational' , 'containerbrand' , 40 ), ( 'C07' , 'Tunnel' , 'Operational' , 'containerbrand' , 40 ), ( 'C08' , 'Tank' , 'Operational' , 'containerbrand' , 40 ), ( 'C09' , 'Thermal' , 'Operational' , 'containerbrand' , 20 ); Configure a Debezium Kafka Connect Connector After you have a configured PostgreSQL instance, you may then configure a new Debezium Kafka Connect connector for PostgreSQL. Kafka Connect connectors are configured by making a request to the Kafka Connect API endpoint. The following shows a configuration of a PostgreSQL instance at postgres-postgresql.your-namespace-here.svc.cluster.local on port 5432 with username postgres and password passw0rd . The database is postgres with table public.containers . In this example, we are using the pgoutput logical decoding output plugin. Also, we are using the cURL HTTP client to make a call to the Kafka Connect API at http://192.168.10.2:30500 since 192.168.10.2 is one our cluster nodes and the Kafka Connect API is exposed as a NodePort on port 30500 . curl -X POST \\ http://192.168.10.2:30500/connectors \\ -H 'Content-Type: application/json' \\ -d '{ \"name\": \"containers-connector\", \"config\": { \"connector.class\": \"io.debezium.connector.postgresql.PostgresConnector\", \"plugin.name\": \"pgoutput\", \"database.hostname\": \"postgres-postgresql.your-namespace-here.svc.cluster.local\", \"database.port\": \"5432\", \"database.user\": \"postgres\", \"database.password\": \"passw0rd\", \"database.dbname\": \"postgres\", \"database.server.name\": \"postgres\", \"table.whitelist\": \"public.containers\" } }' You should then be able to return to the Event Streams UI, navigating to the \"Topics\" section. You can then click on \"postgres.public.containers\" (or whatever your table name is) to see your newly created topic. Then, click on \"Messages\" to view the messages that Debezium has written to the topic.","title":"Event Streams - Debezium"},{"location":"eventstreams-debezium/#using-debezium-with-event-streams-on-openshift","text":"This guide will show you how to use Debezium with Event Streams. It will do this by demonstrating streaming PostgreSQL changes to an Event Streams topic. This guide assumes you have the following: Installation of Event Streams on OpenShift OpenShift CLI & Helm CLI on your client machine Docker installed on your client machine","title":"Using Debezium with Event Streams on OpenShift"},{"location":"eventstreams-debezium/#download-the-kafka-connect-configuration-from-event-streams-configure-event-streams","text":"This section will guide you through downloading the Kafka Connect configuration generated by Event Streams. Log in to the IBM Cloud Private Web Console & navigate to your Event Streams Helm Release from Workloads -> Helm Releases. Launch the Event Streams UI - Inside of the Event Streams Helm Release view, click the Launch button and select admin-ui-https . From the Event Streams UI, Navigate to Toolbox -> Set up a Kafka Connect environment and follow the instructions on screen. These instructions should guide you through creating topics for Kafka Connect, generating an API key and provide you with your Kafka Connect configuration.","title":"Download the Kafka Connect Configuration from Event Streams &amp; Configure Event Streams"},{"location":"eventstreams-debezium/#build-the-kafka-connect-image","text":"This section will guide you through building a Kafka Connect Image that includes the Debezium PostgreSQL connector. This section will use the package downloaded in the previous section. Unzip the kafkaconnect.zip file that was downloaded $ unzip kafkaconnect.zip -d kafkaconnect Delete the zipped kafkaconnect package $ rm kafkaconnect.zip Download the Debezium PostgreSQL Connector (v0.10.0.CR1) package from Maven Central $ wget https://repo1.maven.org/maven2/io/debezium/debezium-connector-postgres/0.10.0.CR1/debezium-connector-postgres-0.10.0.CR1-plugin.tar.gz Untar the Debezium PostgreSQL Connector into the kafkaconnect/connectors directory $ tar -xvzf debezium-connector-postgres-0.10.0.CR1-plugin.tar.gz -C kafkaconnect/connectors Delete the Debezium PostgreSQL Connector tarball $ rm debezium-connector-postgres-0.10.0.CR1-plugin.tar.gz Build the Kafka Connect image with Docker $ docker build -t kafka-connect-debezium-postgres:0.0.1 kafkaconnect","title":"Build the Kafka Connect Image"},{"location":"eventstreams-debezium/#upload-the-kafka-connect-image-to-openshift","text":"This section will guide you through uploading the Kafka Connect image to OpenShift. This will require SSH access to a cluster node. For alternative methods, please refer to the OpenShift documentation . Package the Docker image built in the last section as a tarball $ docker save kafka-connect-debezium-postgres:0.0.1 --output kafka-connect-debezium-postgres-0-0-1.tar Upload the image tarball to one of the cluster nodes using SCP. For example a cluster node at 192.168.10.2 using user root $ scp kafka-connect-debezium-postgres-0-0-1.tar root@192.168.10.2:/root/ SSH into the cluster node $ ssh root@192.168.10.2 Load the uploaded image tarball into the local Docker repository $ docker load --input kafka-connect-debezium-postgres-0-0-1.tar Login to your OpenShift cluster with the oc command $ oc login Login to the OpenShift Docker registry, this assumes that the registry is installed at the default location docker-registry.default.svc.cluster.local:5000 $ docker login -u any_value -p $( oc whoami -t ) docker-registry.default.svc.cluster.local:5000 Tag the Kafka Connect image with an appropriate registry location. Replace your-namespace-here with the namespace where you will deploy Kafka Connect, if deploying to another namespace $ docker tag kafka-connect-debezium-postgres:0.0.1 docker-registry.default.svc.cluster.local:5000/your-namespace-here/kafka-connect-debezium-postgres:0.0.1 Push the image up to the OpenShift Docker registry, replacing your-namespace-here with the namespace you specified in the last step $ docker push docker-registry.default.svc.cluster.local:5000/your-namespace-here/kafka-connect-debezium-postgres:0.0.1","title":"Upload the Kafka Connect Image to OpenShift"},{"location":"eventstreams-debezium/#deploy-kafka-connect-on-openshift","text":"This section will guide you through deploying Kafka Connect on OpenShift using the image that you uploaded in the last section. Kafka Connect will be exposed as a NodePort service on port 30500 of your cluster nodes. Return to the workspace that contains the kafkaconnect directory that was downloaded in the first section, and login to your OpenShift cluster $ oc login Upload connect-distributed.properties as a Secret in the OpenShift namespace where you will deploy Kafka Connect (replacing your-namespace-here ) $ oc create secret generic --namespace your-namespace-here --from-file kafkaconnect/config/connect-distributed.properties connect-distributed-config Upload connect-log4j.properties as a ConfigMap in the OpenShift namespace where you will deploy Kafka Connect (replacing your-namespace-here ) $ oc create configmap --namespace your-namespace-here --from-file kafkaconnect/config/connect-log4j.properties connect-log4j-config Create a file in your workspace named kafka-connect-deploy.yaml with the following contents. Replace the value of spec.template.spec.containers.0.image with the image name that you uploaded in the last section # Deployment apiVersion : apps/v1 kind : Deployment metadata : name : kafkaconnect-deploy labels : app : kafkaconnect spec : replicas : 1 selector : matchLabels : app : kafkaconnect template : metadata : labels : app : kafkaconnect spec : securityContext : runAsNonRoot : true runAsUser : 5000 containers : - name : kafkaconnect-container image : docker-registry.default.svc.cluster.local:5000/your-namespace-here/kafka-connect-debezium-postgres:0.0.1 readinessProbe : httpGet : path : / port : 8083 livenessProbe : httpGet : path : / port : 8083 ports : - containerPort : 8083 volumeMounts : - name : connect-config mountPath : /opt/kafka/config/connect-distributed.properties subPath : connect-distributed.properties - name : connect-log4j mountPath : /opt/kafka/config/connect-log4j.properties subPath : connect-log4j.properties volumes : - name : connect-config secret : secretName : connect-distributed-config - name : connect-log4j configMap : name : connect-log4j-config --- # Service apiVersion : v1 kind : Service metadata : name : kafkaconnect-service labels : app : kafkaconnect-service spec : type : NodePort ports : - name : kafkaconnect protocol : TCP port : 8083 nodePort : 30500 selector : app : kafkaconnect Deploy Kafka Connect (replacing your-namespace-here ) $ oc create --namespace your-namespace-here -f kafka-connect-deploy.yaml","title":"Deploy Kafka Connect on OpenShift"},{"location":"eventstreams-debezium/#configure-or-install-postgresql","text":"This section will give you the minimum required configuration for PostgreSQL by Debezium. If you do not have PostgreSQL, you can proceed to the Installation section.","title":"Configure or Install PostgreSQL"},{"location":"eventstreams-debezium/#configure-postgresql","text":"If you already have PostgreSQL installed, you must enable the logical decoding feature of PostgreSQL (v. >=9.4) and enable a logical decoding output plugin. PostgreSQL v.10+ includes the pgoutput logical decoding output plugin by default. If you are on a lower version, you must first install either decoderbufs or wal2json . More information can be found on the Debezium website here After you have an acceptable local decoding output plugin installed, refer to the Debezium documentation to complete the configuration of your PostgreSQL server https://debezium.io/documentation/reference/0.10/connectors/postgresql.html#server-configuration","title":"Configure PostgreSQL"},{"location":"eventstreams-debezium/#install-postgresql","text":"If you do not have PostgreSQL installed, you may use a Helm Chart to get a working PostgreSQL instance on your cluster. This section will guide you through the installation of PostgreSQL using the Stable Helm Chart. You will also add some additional configuration for PostgreSQL necessary for Debezium to read the PostgreSQL transaction log. PostgreSQL will be available on port 30600 of your cluster nodes. The superuser will be postgres , password is passw0rd and an initial database name of postgres . Create a PostgreSQL configuration necessary for Debezium. Create a file in your workspace named extended.conf with the following contents: # extended . conf wal_level = logical max_wal_senders = 1 max_replication_slots = 1 Create a ConfigMap from the extended.conf file with the following command (replacing your-namespace-here ) $ oc create configmap --namespace your-namepace-here --from-file = extended.conf postgresql-config Install PostgreSQL using the Stable Helm Chart with the following command (replacing your-namespace-here ). For more configuration options, refer to the PostgreSQL Helm Chart $ helm install --name postgres --namespace your-namespace-here --set extendedConfConfigMap = postgresql-config --set service.type = NodePort --set service.nodePort = 30600 --set postgresqlPassword = passw0rd --set securityContext.runAsUser = 5000 --set volumePermissions.securityContext.runAsUser = 5000 stable/postgresql --tls Finally, connect to your PostgreSQL instance with your favorite PostgreSQL client and create a table with data. Note that the psql client is installed on the postgres container. You may exec into the postgres container and use psql from there. $ oc exec --namespace your-namespace-here -it postgres-postgresql-0 -- /bin/sh $ psql login --user postgres","title":"Install PostgreSQL"},{"location":"eventstreams-debezium/#add-sample-data-to-postgresql","text":"Open a shell in the Postgres container. $ kubectl exec --namespace your-namespace-here -it postgres-postgresql-0 -- /bin/sh Login to Postgres with the following command, entering the password passw0rd when prompted (this is the password set from the helm install command in the previous section). $ psql --user postgres Create a table named containers . CREATE TABLE containers ( containerid VARCHAR ( 30 ) NOT NULL , type VARCHAR ( 20 ), status VARCHAR ( 20 ), brand VARCHAR ( 50 ), capacity DECIMAL , CREATIONDATE TIMESTAMP DEFAULT CURRENT_TIMESTAMP , UPDATEDATE TIMESTAMP DEFAULT CURRENT_TIMESTAMP , PRIMARY KEY ( containerid )); Insert data into the table. INSERT INTO containers ( containerid , type , status , brand , capacity ) VALUES ( 'C01' , 'Reefer' , 'Operational' , 'containerbrand' , 20 ), ( 'C02' , 'Dry' , 'Operational' , 'containerbrand' , 20 ), ( 'C03' , 'Dry' , 'Operational' , 'containerbrand' , 40 ), ( 'C04' , 'FlatRack' , 'Operational' , 'containerbrand' , 40 ), ( 'C05' , 'OpenTop' , 'Operational' , 'containerbrand' , 40 ), ( 'C06' , 'OpenSide' , 'Operational' , 'containerbrand' , 40 ), ( 'C07' , 'Tunnel' , 'Operational' , 'containerbrand' , 40 ), ( 'C08' , 'Tank' , 'Operational' , 'containerbrand' , 40 ), ( 'C09' , 'Thermal' , 'Operational' , 'containerbrand' , 20 );","title":"Add Sample Data to PostgreSQL"},{"location":"eventstreams-debezium/#configure-a-debezium-kafka-connect-connector","text":"After you have a configured PostgreSQL instance, you may then configure a new Debezium Kafka Connect connector for PostgreSQL. Kafka Connect connectors are configured by making a request to the Kafka Connect API endpoint. The following shows a configuration of a PostgreSQL instance at postgres-postgresql.your-namespace-here.svc.cluster.local on port 5432 with username postgres and password passw0rd . The database is postgres with table public.containers . In this example, we are using the pgoutput logical decoding output plugin. Also, we are using the cURL HTTP client to make a call to the Kafka Connect API at http://192.168.10.2:30500 since 192.168.10.2 is one our cluster nodes and the Kafka Connect API is exposed as a NodePort on port 30500 . curl -X POST \\ http://192.168.10.2:30500/connectors \\ -H 'Content-Type: application/json' \\ -d '{ \"name\": \"containers-connector\", \"config\": { \"connector.class\": \"io.debezium.connector.postgresql.PostgresConnector\", \"plugin.name\": \"pgoutput\", \"database.hostname\": \"postgres-postgresql.your-namespace-here.svc.cluster.local\", \"database.port\": \"5432\", \"database.user\": \"postgres\", \"database.password\": \"passw0rd\", \"database.dbname\": \"postgres\", \"database.server.name\": \"postgres\", \"table.whitelist\": \"public.containers\" } }' You should then be able to return to the Event Streams UI, navigating to the \"Topics\" section. You can then click on \"postgres.public.containers\" (or whatever your table name is) to see your newly created topic. Then, click on \"Messages\" to view the messages that Debezium has written to the topic.","title":"Configure a Debezium Kafka Connect Connector"},{"location":"mq-kafka/","text":"MQ to Kafka integration","title":"MQ - Kafka"},{"location":"mq-kafka/#mq-to-kafka-integration","text":"","title":"MQ to Kafka integration"},{"location":"openshift/","text":"Deploy and run on Openshift We address here how to deploy the liberty app to Openshift connected to Db2 on IBM Cloud and MQ on IBM Cloud. Pre-requisite You should have already provisionned Db2 as service on Cloud, if not see this section . Get a DB2 server or service up and running. You can create a DB2 service on IBM Cloud using the product documentation instructions . Get the service credentials. Install DB driver on your local environment by following one of those instructions . Basically, open the DB2 console, under the burger icon go to Connection Information and select the operating system needed. Set up the environment with the db2profile: source /Applications/dsdriver/db2profile . Create tables and load data Using the DB2 console, use Run SQL feature, to create the containers table (create statement in the file db2/sql/inventory/create-tables.db2 ) and load the data using the SQL file (db2/sql/inventory/loaddata.db2)","title":"Deploy and run on Openshift"},{"location":"openshift/#deploy-and-run-on-openshift","text":"We address here how to deploy the liberty app to Openshift connected to Db2 on IBM Cloud and MQ on IBM Cloud.","title":"Deploy and run on Openshift"},{"location":"openshift/#pre-requisite","text":"You should have already provisionned Db2 as service on Cloud, if not see this section . Get a DB2 server or service up and running. You can create a DB2 service on IBM Cloud using the product documentation instructions . Get the service credentials. Install DB driver on your local environment by following one of those instructions . Basically, open the DB2 console, under the burger icon go to Connection Information and select the operating system needed. Set up the environment with the db2profile: source /Applications/dsdriver/db2profile .","title":"Pre-requisite"},{"location":"openshift/#create-tables-and-load-data","text":"Using the DB2 console, use Run SQL feature, to create the containers table (create statement in the file db2/sql/inventory/create-tables.db2 ) and load the data using the SQL file (db2/sql/inventory/loaddata.db2)","title":"Create tables and load data"},{"location":"run-local/","text":"Run the solution locally This is for development by running Db2 in docker on your laptop. Run the Liberty server locally with DB2 and MQ in docker containers. Pre-requisites Get docker and docker cli for your local environment Get docker compose Creating the INVDB in DB2 Note The creation of the INVDB database with the containers table and some test data should be done only the first time, or each time you delete the db2/database folder. Go to the db2 folder and start the db2 server with the script: ./startDB2.sh # which is the same as running the ibmcom/db2 community edition docker run -it --name db2 --privileged = true -p 50000 :50000 -e LICENSE = accept -e DB2INST1_PASSWORD = db2inst1 -e DBNAME = INVDB -v $( pwd ) :/home -v $( pwd ) /database:/database ibmcom/db2 This will download a development docker image for DB2 and configure the INVDB database. The container table needs to be added using the following steps within the docker shell: Start the bash session within the db2 server docker exec - ti db2 bash Then swap to db2inst1 user and use the scripts that were mounted under /home su - db2inst1 cd / home / sql / inventory . / createDB . sh The trace should look like : ``` INVDB Database Connection Information ... Containers table not found so let create it create table containers ( containerid VARCHAR ( 30 ) NOT NULL , type VARCHAR ( 20 ), status VARCHAR ( 20 ), brand VARCHAR ( 50 ), capacity DECIMAL , CREATIONDATE TIMESTAMP , UPDATEDATE TIMESTAMP , PRIMARY KEY ( containerid ) ) DB20000I The SQL command completed successfully . There is no data in inventory DB , let add some ... INSERT INTO containers ( containerid , type , status , brand , capacity ) VALUES ( 'C01' , 'Reefer' , 'Operational' , 'containerbrand' , 20 ), ( 'C02' , 'Dry' , 'Operational' , 'containerbrand' , 20 ), ( 'C03' , 'Dry' , 'Operational' , 'containerbrand' , 40 ), ( 'C04' , 'FlatRack' , 'Operational' , 'containerbrand' , 40 ), ( 'C05' , 'OpenTop' , 'Operational' , 'containerbrand' , 40 ), ( 'C06' , 'OpenSide' , 'Operational' , 'containerbrand' , 40 ), ( 'C07' , 'Tunnel' , 'Operational' , 'containerbrand' , 40 ), ( 'C08' , 'Tank' , 'Operational' , 'containerbrand' , 40 ), ( 'C09' , 'Thermal' , 'Operational' , 'containerbrand' , 20 ) DB20000I The SQL command completed successfully . ``` Exit the shell sessions. The database is created under the db2 folder, and it is git ignored. Stop DB2 docker instance with docker stop db2 Tag the image of the container so we can reuse the same database. ID = $ ( docker ps - l | grep db2 | awk '{print $1}' ) docker commit --author=\"IBMCASE\" $ID ibmcase/greendb2 This docker image will be used in the docker compose settings to run the solution locally. Also the ibncase/greendb2 is available in public docker hub. Create the MQ image with the queue configuration Note Queue manager and queue data are saved in the filesystem. To avoid losing the queue manager and queue data, we use docker volumes. Volumes are attached to containers when they are run and persist after the container is deleted. The following command creates a volume name qm1data Work under the mq-config folder. cd mq-config docker volume create qm1data The remote MQ clients use a Channel to communicate with the MQ manager over the network. We need to create a virtual docker network to support this communication. The command below creates such network: docker network create mq-network The folder mq-config includes the 20-config.mqsc file to define the queues to be used to communicate with Kafka. define ql ( 'TO.CONTAINERS' ) define ql ( 'FROM.CONTAINERS' ) The goal is to take the IBM MQ development image and move this configuration file inside the image. docker build - t ibmcase / mq . For more information of the IBM MQ image see this note and how to use the MQ container . The scripts runMQlocal.sh uses docker and the IBM MQ docker image to run MQ as a daemon. This is helpful when developing the JMS consumer and producer code, but we recommend to use the docker-compose to start both MQ and DB2. Going to the console: https://localhost:9443/ibmmq/console/login.html to login using the admin user. See instructions in this note . Build the jee-inventory webapp mvn package - DskipIT This will build the containerinventory.war file and prepare Liberty defaultServer without running the Integration tests. Start the webapp with maven To start the web app mvn liberty : run Point the browser to the following address: Verify the app is up and running http://localhost:9080/health Get the list of all containers: http://localhost:9080/containers Get a container using its ID: http://localhost:9080/containers/C02 Get the private api https://localhost:9443/ibm/api use the admin user and password as defined in the src/main/liberty/config/server.xml file. Start the solution using docker compose Under the project folder, use the following command to start both DB2 and MQ docker - compose up The docker-compose.yml file defines the two services, one volume for MQ and the mq network.","title":"Deploy and run locally"},{"location":"run-local/#run-the-solution-locally","text":"This is for development by running Db2 in docker on your laptop. Run the Liberty server locally with DB2 and MQ in docker containers.","title":"Run the solution locally"},{"location":"run-local/#pre-requisites","text":"Get docker and docker cli for your local environment Get docker compose","title":"Pre-requisites"},{"location":"run-local/#creating-the-invdb-in-db2","text":"Note The creation of the INVDB database with the containers table and some test data should be done only the first time, or each time you delete the db2/database folder. Go to the db2 folder and start the db2 server with the script: ./startDB2.sh # which is the same as running the ibmcom/db2 community edition docker run -it --name db2 --privileged = true -p 50000 :50000 -e LICENSE = accept -e DB2INST1_PASSWORD = db2inst1 -e DBNAME = INVDB -v $( pwd ) :/home -v $( pwd ) /database:/database ibmcom/db2 This will download a development docker image for DB2 and configure the INVDB database. The container table needs to be added using the following steps within the docker shell: Start the bash session within the db2 server docker exec - ti db2 bash Then swap to db2inst1 user and use the scripts that were mounted under /home su - db2inst1 cd / home / sql / inventory . / createDB . sh The trace should look like : ``` INVDB Database Connection Information ... Containers table not found so let create it create table containers ( containerid VARCHAR ( 30 ) NOT NULL , type VARCHAR ( 20 ), status VARCHAR ( 20 ), brand VARCHAR ( 50 ), capacity DECIMAL , CREATIONDATE TIMESTAMP , UPDATEDATE TIMESTAMP , PRIMARY KEY ( containerid ) ) DB20000I The SQL command completed successfully . There is no data in inventory DB , let add some ... INSERT INTO containers ( containerid , type , status , brand , capacity ) VALUES ( 'C01' , 'Reefer' , 'Operational' , 'containerbrand' , 20 ), ( 'C02' , 'Dry' , 'Operational' , 'containerbrand' , 20 ), ( 'C03' , 'Dry' , 'Operational' , 'containerbrand' , 40 ), ( 'C04' , 'FlatRack' , 'Operational' , 'containerbrand' , 40 ), ( 'C05' , 'OpenTop' , 'Operational' , 'containerbrand' , 40 ), ( 'C06' , 'OpenSide' , 'Operational' , 'containerbrand' , 40 ), ( 'C07' , 'Tunnel' , 'Operational' , 'containerbrand' , 40 ), ( 'C08' , 'Tank' , 'Operational' , 'containerbrand' , 40 ), ( 'C09' , 'Thermal' , 'Operational' , 'containerbrand' , 20 ) DB20000I The SQL command completed successfully . ``` Exit the shell sessions. The database is created under the db2 folder, and it is git ignored. Stop DB2 docker instance with docker stop db2 Tag the image of the container so we can reuse the same database. ID = $ ( docker ps - l | grep db2 | awk '{print $1}' ) docker commit --author=\"IBMCASE\" $ID ibmcase/greendb2 This docker image will be used in the docker compose settings to run the solution locally. Also the ibncase/greendb2 is available in public docker hub.","title":"Creating the INVDB in DB2"},{"location":"run-local/#create-the-mq-image-with-the-queue-configuration","text":"Note Queue manager and queue data are saved in the filesystem. To avoid losing the queue manager and queue data, we use docker volumes. Volumes are attached to containers when they are run and persist after the container is deleted. The following command creates a volume name qm1data Work under the mq-config folder. cd mq-config docker volume create qm1data The remote MQ clients use a Channel to communicate with the MQ manager over the network. We need to create a virtual docker network to support this communication. The command below creates such network: docker network create mq-network The folder mq-config includes the 20-config.mqsc file to define the queues to be used to communicate with Kafka. define ql ( 'TO.CONTAINERS' ) define ql ( 'FROM.CONTAINERS' ) The goal is to take the IBM MQ development image and move this configuration file inside the image. docker build - t ibmcase / mq . For more information of the IBM MQ image see this note and how to use the MQ container . The scripts runMQlocal.sh uses docker and the IBM MQ docker image to run MQ as a daemon. This is helpful when developing the JMS consumer and producer code, but we recommend to use the docker-compose to start both MQ and DB2. Going to the console: https://localhost:9443/ibmmq/console/login.html to login using the admin user. See instructions in this note .","title":"Create the MQ image with the queue configuration"},{"location":"run-local/#build-the-jee-inventory-webapp","text":"mvn package - DskipIT This will build the containerinventory.war file and prepare Liberty defaultServer without running the Integration tests.","title":"Build the jee-inventory webapp"},{"location":"run-local/#start-the-webapp-with-maven","text":"To start the web app mvn liberty : run Point the browser to the following address: Verify the app is up and running http://localhost:9080/health Get the list of all containers: http://localhost:9080/containers Get a container using its ID: http://localhost:9080/containers/C02 Get the private api https://localhost:9443/ibm/api use the admin user and password as defined in the src/main/liberty/config/server.xml file.","title":"Start the webapp with maven"},{"location":"run-local/#start-the-solution-using-docker-compose","text":"Under the project folder, use the following command to start both DB2 and MQ docker - compose up The docker-compose.yml file defines the two services, one volume for MQ and the mq network.","title":"Start the solution using docker compose"}]}